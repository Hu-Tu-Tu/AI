[TOC]

# 第一次实验报告目录

## 线性回归验证实验

```python
# 线性回归模型的应用
from sklearn import linear_model
regression=linear_model.LinearRegression() # 创建线性回归模型
X=[[3],[8]] # 观察值的X坐标，是一个二维的数组
y=[1,2] # 观察值的y坐标，是一个一维数组
regression.fit(X,y) # 拟合
```

LinearRegression()

```python
regression.intercept_  # 截距
```

0.40000000000000013

```python
regression.coef_ # 斜率，回归系数
                 # 反映x对y影响的大小
```

array([0.2])

- `np.random.randint(随机最小值，随机最大值，[生成多少行，多少列]）`
- 使用以上生成10个随机数据，用来测试

```python
# regression.predict([[6]])
unKnown=np.random.randint(0,100,[10,1])
regression.predict(unKnown)
```

- **结果截图：**

![image-20220319140354238](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220319140354238.png)



## 岭回归算法验证实验

- 岭回归算法实验与线性回归算法的实验数据是一样的，两者只有模型的不同，但是在一定情况下两者可以转化。

- 岭回归通过在代价函数后面加上一个对参数的约束项来防止过拟合，其中约束项系数α数值越大，特征对结果的影响就越小。

> ==约束项系数为10==

```python
from sklearn.linear_model import Ridge
import numpy as np
ridgeRegression=Ridge(alpha=10) # 创建岭回归模型，设置约束项系数为10
X=[[3],[8]]
y=[1,2]
ridgeRegression.fit(X,y) # 拟合
```

Ridge(alpha=10)

```python
unKnown=np.random.randint(0,100,[10,1])
ridgeRegression.predict(unKnown) # 预测
```

array([7.        , 9.88888889, 7.77777778, 9.22222222, 7.66666667,
      		 8.44444444, 8.        , 9.11111111, 9.        , 9.77777778])

```
# 查看回归系数
ridgeRegression.coef_
```

array([0.11111111])

```
# 和截距
ridgeRegression.intercept_
```

0.8888888888888888

- **结果截图：**

![image-20220319141230138](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220319141230138.png)

---

---

> ==约束项是1==

```
from sklearn.linear_model import Ridge
ridgeRegression=Ridge(alpha=1.0) 
X=[[3],[8]]
y=[1,2]
ridgeRegression.fit(X,y) # 拟合
```

Out[9]:Ridge()

```
ridgeRegression.predict([[6]]) # 预测
```

Out[10]:array([1.59259259])

```
# 查看回归系数
ridgeRegression.coef_
```

Out[11]:array([0.18518519])

```
# 和截距
ridgeRegression.intercept_
```

Out[12]:0.4814814814814816

- **结果截图：**

![image-20220317120430723](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220317120430723.png)

---

---

> ==约束项是0==

- 当约束项为0的时候，我们比较和线性回归验证函数的数据发现，这个时候的岭回归等价于线性回归

```python
from sklearn.linear_model import Ridge
ridgeRegression=Ridge(alpha=0) 
X=[[3],[8]]
y=[1,2]
ridgeRegression.fit(X,y) # 拟合
```

Out[13]:Ridge(alpha=0)

```
ridgeRegression.predict([[6]]) # 预测
```

Out[14]:array([1.6])

```
# 查看回归系数
ridgeRegression.coef_
```

Out[15]:array([0.2])

```
# 和截距
ridgeRegression.intercept_
```

Out[16]:0.40000000000000013

- **结果截图：**

![image-20220317120726216](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220317120726216.png)

---

---

> ==指定约束项的范围==

```python
import numpy as np
from sklearn.linear_model import RidgeCV
X=[[3],[8]]
y=[1,2]
reg=RidgeCV(alphas=np.arange(0.001,10)) # 指定α参数的范围，不能是负数和0，每次递增为1
reg.fit(X,y) # 拟合
```

Out[17]:RidgeCV(alphas=array([1.000e-03, 1.001e+00, 2.001e+00, 3.001e+00, 4.001e+00, 5.001e+00,
       6.001e+00, 7.001e+00, 8.001e+00, 9.001e+00]))

```python
reg.alpha_ # 最佳数值，拟合之后这个值才可以使用
```

Out[18]:2.001

```
reg.predict([[6]]) # 预测
```

Out[19]:array([1.58620095])

- **结果截图：**

![image-20220317121003352](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220317121003352.png)

---

---

## Lasso回归算法验证实验

> ==惩罚系数是3.0==

```python
from sklearn.linear_model import Lasso
X=[[3],[8]]
y=[1,2]

reg=Lasso(alpha=3.0) # 惩罚系数是3.0
reg.fit(X,y) # 拟合
```

Out[20]:Lasso(alpha=3.0)

```
reg.coef_ # 查看系数
```

Out[21]:array([0.])

```
reg.intercept_ # 截距
```

Out[22]:1.5

```
reg.predict([[6]])
```

Out[23]:array([1.5])

- **结果截图：**

![image-20220317231433028](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220317231433028.png)

## 弹性网络算法验证实验

> ==alpha=1.0,l1_ratio=0.7==

```python
from sklearn.linear_model import ElasticNet
reg=ElasticNet(alpha=1.0,l1_ratio=0.7)
X=[[3],[8]]
y=[1,2]
# 拟合
reg.fit(X,y)
```

Out[1]:ElasticNet(l1_ratio=0.7)

```
# 预测
reg.predict([[6]])
```

Out[2]:array([1.54198473])

```
reg.coef_
```

Out[3]:array([0.08396947])

```
reg.intercept_
```

Out[4]:1.0381679389312977

- **结果截图：**

![image-20220317232142385](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220317232142385.png)



## 回归算法来训练模型预测儿童身高

- **首先我们先导入数据，并且使用sklearn中的模型来训练数据，这里值得注意的是，我们设定年龄在18岁之后身高便不随年龄增长而变高。**

```python
import copy
import numpy as np
from sklearn import linear_model

# 训练数据，每一行表示一个样本，包含的信息分别为：
# 儿童年龄,性别（0女1男）
# 父亲、母亲、祖父、祖母、外祖父、外祖母的身高
X = np.array([[1, 0, 180, 165, 175, 165, 170, 165],
              [3, 0, 180, 165, 175, 165, 173, 165],
              [4, 0, 180, 165, 175, 165, 170, 165],
              [6, 0, 180, 165, 175, 165, 170, 165],
              [8, 1, 180, 165, 175, 167, 170, 165],
              [10, 0, 180, 166, 175, 165, 170, 165],
              [11, 0, 180, 165, 175, 165, 170, 165],
              [12, 0, 180, 165, 175, 165, 170, 165],
              [13, 1, 180, 165, 175, 165, 170, 165],
              [14, 0, 180, 165, 175, 165, 170, 165],
              [17, 0, 170, 165, 175, 165, 170, 165]])

# 儿童身高，单位：cm
y = np.array([60, 90, 100, 110, 130, 140, 150, 164, 160, 163, 168])

# 创建线性回归模型
lr=linear_model.LinearRegression()
# 拟合
lr.fit(X,y)
```

Out[5]:LinearRegression()

- 输入测试数据

```python
# 待测的未知数据，其中每个分量的含义和训练数据相同
x = np.array([[10, 0, 180, 165, 175, 165, 170, 165],
               [17, 1, 173, 153, 175, 161, 170, 161],
               [34, 0, 170, 165, 170, 165, 170, 165]])

for item in x:
    # 为不改变原始数据，进行深复制，并假设超过18岁以后就不再长高了
    # 对于18岁以后的年龄，返回18岁时的身高
    item1 = copy.deepcopy(item)
    if item1[0] > 18:
        item1[0] = 18
    print(item, ':', lr.predict(item1.reshape(1,-1)))
```

[ 10   0 180 165 175 165 170 165] : [140.56153846]
		[ 17   1 173 153 175 161 170 161] : [158.41]
		[ 34   0 170 165 170 165 170 165] : [176.03076923]

- 我们的输出结果是测试数据：预测身高
- **结果截图**

![image-20220317233028752](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220317233028752.png)



## 逻辑回归算法预测考试是否能够及格

> ==随机预测==

- **随机预测的特点在于使用了`random`函数，模拟数据的随机性**

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

# 构造测试数据
X=np.array([[i] for i in range(30)])
y=np.array([0]*15+[1]*15)

# 人为修改部分样本的值
y[np.random.randint(0,15,3)] = 1
y[np.random.randint(15,30,4)] = 0
print(y[:15])
print(y[15:])
```

[1 0 0 0 0 0 0 0 0 0 0 1 0 0 1]
		[1 1 1 1 1 1 0 1 1 1 1 0 0 1 1]

```python
# 根据原始数据绘制散点图
plt.scatter(X,y)
```

Out[8]:<matplotlib.collections.PathCollection at 0x244ae0fd808>![image-20220317234247019](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220317234247019.png)

```python
# 创建并训练逻辑回归模型
reg=LogisticRegression('l2',C=3.0)
reg.fit(X,y)
```

Out[9]:LogisticRegression(C=3.0)

```python
# 对未知数据进行预测
print(reg.predict([[5],[19]]))
```

[0 1]

```python
# 未知数据属于某一种类别的概率
print(reg.predict_proba([[5],[19]]))
```

[[0.78130853 0.21869147]
 		[0.35362437 0.64637563]]



- **把原始数据散点图和预测结果散点图合并一起画出对比**

```python
# 对原始观察点进行预测
yy=reg.predict(X)
# 根据原始数据绘制散点图
plt.scatter(X,y)
# 根据预测结果绘制折线图
plt.plot(X,yy)
plt.show()
```

![image-20220317234439185](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220317234439185.png)

> ==使用逻辑回归模型算法预测考试是否能及格==

```python
from sklearn.linear_model import LogisticRegression

# 复习情况，格式为(时长,效率)，其中时长单位为小时
# 效率为[0,1]之间的小数，数值越大表示效率越高
X_train = [(0,0), (2,0.9), (3,0.4), (4,0.9), (5,0.4), (6,0.4), (6,0.8), (6,0.7), (7,0.2), (7.5,0.8),(7,0.9), (8,0.1), (8,0.6), (8,0.8)]
# 0表示不及格，1表示及格
y_train = [0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1]

# 创建并训练逻辑回归模型
reg=LogisticRegression()
reg.fit(X_train,y_train)
```

Out[13]:LogisticRegression()

```python
# 测试模型
X_test = [(3,0.9), (8,0.5), (7,0.2), (4,0.5), (4,0.7)]
y_test = [0, 1, 0, 0, 1]
score = reg.score(X_test, y_test)

# 预测并输出预测结果
learning = [(8, 0.9)]
result = reg.predict_proba(learning)
msg = '''模型得分：{0}
复习时长为：{1[0]}，效率为：{1[1]}
您不及格的概率为：{2[0]}
您及格的概率为：{2[1]}
综合判断，您会：{3}'''.format(score, learning[0], result[0], '不及格' if result[0][0]>0.5 else '及格')
print(msg)
```

模型得分：0.6
		复习时长为：8，效率为：0.9
		您不及格的概率为：0.18982398713996873
		您及格的概率为：0.8101760128600313
		综合判断，您会：及格

- **结果截图**

![image-20220317234632653](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220317234632653.png)

## 朴素贝叶斯算法对中文邮件进行分类

**程序逻辑：**

1. 首先是获取邮箱中的所有词语，并且过滤掉所有的干扰字符和无效字符，并且把长度为1的词也过滤掉，得到有效的词语。

```python
# 获取每一封邮件中的所有词语
def getWordsFromFile(txtFile):
    words = []
    # 所有存储邮件文本内容的记事本文件都使用UTF8编码
    with open(txtFile, encoding='utf8') as fp:
        for line in fp:
            # 遍历每一行，删除两端的空白字符
            line = line.strip()
            # 过滤干扰字符或无效字符
            line = sub(r'[.【】0-9、—。，！~\*]', '', line)
            # 分词
            line = cut(line)
            # 过滤长度为1的词
            line = filter(lambda word: len(word)>1, line)
            # 把本行文本预处理得到的词语添加到words列表中
            words.extend(line)
    # 返回包含当前邮件文本中所有有效词语的列表
    return words
```



2. 存放所有的单词,返回出现次数最多的前600个单词，这里我使用的是绝对路径。

```python
# 存放所有文件中的单词
# 每个元素是一个子列表，其中存放一个文件中的所有单词
allWords = []
def getTopNWords(topN):
    # 按文件编号顺序处理当前文件夹中所有记事本文件
    # 训练集中共151封邮件内容，0.txt到126.txt是垃圾邮件内容，127.txt到150.txt为正常邮件内容
    txtFiles = ['D:\\workspaces\\AI\ip\\人工智能\\数据集\\贝叶斯中文邮件分类\\'+ str(i) +'.txt' for i in range(151)]
    # 获取训练集中所有邮件中的全部单词
    for txtFile in txtFiles:
        allWords.append(getWordsFromFile(txtFile))
    # 获取并返回出现次数最多的前topN个单词
    freq = Counter(chain(*allWords))
    return [w[0] for w in freq.most_common(topN)]

# 全部训练集中出现次数最多的前600个单词
topWords = getTopNWords(600)
```

3. 获取特征向量，计算前600个单词在每一个邮件中出现的频率,并且贴上标签

```python
# 获取特征向量，前600个单词的每个单词在每个邮件中出现的频率
vectors = []
for words in allWords:
    temp = list(map(lambda x: words.count(x), topWords))
    vectors.append(temp)

vectors = array(vectors)
# 训练集中每个邮件的标签，1表示垃圾邮件，0表示正常邮件
labels = array([1]*127 + [0]*24)
```



4. 训练模型并且预测测试数据内容

```python
# 创建模型，使用已知训练集进行训练
model = MultinomialNB()
model.fit(vectors, labels)

def predict(txtFile):
    # 获取指定邮件文件内容，返回分类结果
    words = getWordsFromFile(txtFile)
    currentVector = array(tuple(map(lambda x: words.count(x),
                                    topWords)))
    result = model.predict(currentVector.reshape(1, -1))[0]
    print(model.predict_proba(currentVector.reshape(1, -1)))
    return '垃圾邮件' if result==1 else '正常邮件'

# 151.txt至155.txt为测试邮件内容
for mail in ('D:\\workspaces\\AI\ip\\人工智能\\数据集\\贝叶斯中文邮件分类\\'+ str(i) +'.txt' for i in range(151, 156)):
    print(mail, predict(mail), sep=':')
```



**全部代码展示**

```python
from re import sub
from os import listdir
from collections import Counter
from itertools import chain
from numpy import array
from jieba import cut
from sklearn.naive_bayes import MultinomialNB

# 获取每一封邮件中的所有词语
def getWordsFromFile(txtFile):
    words = []
    # 所有存储邮件文本内容的记事本文件都使用UTF8编码
    with open(txtFile, encoding='utf8') as fp:
        for line in fp:
            # 遍历每一行，删除两端的空白字符
            line = line.strip()
            # 过滤干扰字符或无效字符
            line = sub(r'[.【】0-9、—。，！~\*]', '', line)
            # 分词
            line = cut(line)
            # 过滤长度为1的词
            line = filter(lambda word: len(word)>1, line)
            # 把本行文本预处理得到的词语添加到words列表中
            words.extend(line)
    # 返回包含当前邮件文本中所有有效词语的列表
    return words

# 存放所有文件中的单词
# 每个元素是一个子列表，其中存放一个文件中的所有单词
allWords = []
def getTopNWords(topN):
    # 按文件编号顺序处理当前文件夹中所有记事本文件
    # 训练集中共151封邮件内容，0.txt到126.txt是垃圾邮件内容，127.txt到150.txt为正常邮件内容
    txtFiles = ['D:\\workspaces\\AI\ip\\人工智能\\数据集\\贝叶斯中文邮件分类\\'+ str(i) +'.txt' for i in range(151)]
    # 获取训练集中所有邮件中的全部单词
    for txtFile in txtFiles:
        allWords.append(getWordsFromFile(txtFile))
    # 获取并返回出现次数最多的前topN个单词
    freq = Counter(chain(*allWords))
    return [w[0] for w in freq.most_common(topN)]

# 全部训练集中出现次数最多的前600个单词
topWords = getTopNWords(600)

# 获取特征向量，前600个单词的每个单词在每个邮件中出现的频率
vectors = []
for words in allWords:
    temp = list(map(lambda x: words.count(x), topWords))
    vectors.append(temp)

vectors = array(vectors)
# 训练集中每个邮件的标签，1表示垃圾邮件，0表示正常邮件
labels = array([1]*127 + [0]*24)

# 创建模型，使用已知训练集进行训练
model = MultinomialNB()
model.fit(vectors, labels)

def predict(txtFile):
    # 获取指定邮件文件内容，返回分类结果
    words = getWordsFromFile(txtFile)
    currentVector = array(tuple(map(lambda x: words.count(x),
                                    topWords)))
    result = model.predict(currentVector.reshape(1, -1))[0]
    print(model.predict_proba(currentVector.reshape(1, -1)))
    return '垃圾邮件' if result==1 else '正常邮件'

# 151.txt至155.txt为测试邮件内容
for mail in ('D:\\workspaces\\AI\ip\\人工智能\\数据集\\贝叶斯中文邮件分类\\'+ str(i) +'.txt' for i in range(151, 156)):
    print(mail, predict(mail), sep=':')
```

**结果截图**

![image-20220318095106839](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318095106839.png)

==**报错出现：**==

1. 编辑路径的时候出现报错：`EOL while scanning string literal`，发现其实是自己的路径使用的是单行线，网上查之后改为单行线就运行成功了。
2. 导包错误：`No module named 'jieba'`，直接打开jupyter notebook环境运行`pip install jieba`即可

## 决策树算法验证实验

```python
import numpy as np
from sklearn import tree
# 数据
X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])
y=[0,1,1,1,2,3,3,4]
# 创建决策树分类器
clf=tree.DecisionTreeClassifier()
clf.fit(X,y)
```

Out[10]:DecisionTreeClassifier()

```python
# 预测
clf.predict([[1,0,0]])
```

Out[11]:array([2])

```python
import graphviz
# 导出决策树
dot_data=tree.export_graphviz(clf,out_file=None)
# 创建图形
graph= graphviz.Source(dot_data) 
# 输出pdf文件
graph.render('result')
```

Out[4]:'result.pdf'

**结果截图**

![image-20220318100050547](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318100050547.png)

![image-20220316153126307](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220316153126307.png)

==**报错信息**==

`failed to execute [‘dot’, ‘-Tsvg’], make sure the Graphviz executables are on your systems`原因是`graphviz模块`没安装，但是我们直接`pip install graphviz`也是会同样报错的，因为这样下载系统环境变量是没有这个模块的。解决方案是直接到graphviz官网中去下载安装，并且配置好系统环境变量，最后加载到python环境就可以了。

## 判断学员的Python水平

**程序思路**

通过一个问卷调查来输入数据，检验所学的课本以及学习的程度来检验学员的python水平。

1. 训练模型

```python
questions = ('《Python程序设计基础（第2版）》',
             '《Python程序设计基础与应用》',
             '《Python程序设计（第2版）》',
             '《大数据的Python基础》',
             '《Python程序设计开发宝典》',
             '《Python可以这样学》',
             '《中学生可以这样学Python》',
             '《Python编程基础与案例集锦（中学版）》',
             '《玩转Python轻松过二级》',
             '微信公众号“Python小屋”的免费资料',)
# 每个样本的数据含义：
# 0没看过，1很多看不懂，2大部分可以看懂，3没压力
answers = [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3],
           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [0, 0, 0, 0, 0, 0, 2, 2, 0, 1],
           [0, 0, 0, 0, 3, 3, 0, 0, 0, 3],
          
           [3, 3, 0, 3, 0, 0, 0, 0, 3, 1],
           [3, 0, 3, 0, 3, 0, 0, 3, 3, 2],
           [0, 0, 3, 0, 3, 3, 0, 0, 0, 3],
           [2, 2, 0, 2, 0, 0, 0, 0, 0, 1],
           [0, 2, 1, 3, 1, 1, 0, 0, 2, 1]
          ]

labels = ['超级高手', '门外汉', '初级选手', '初级选手', '高级选手',
          '中级选手', '高级选手', '超级高手', '初级选手', '初级选手']

clf = tree.DecisionTreeClassifier().fit(answers, labels) # 训练
```

2. 设计调查问卷并且分类

```python
yourAnswer = []
# 显示调查问卷，并接收用户输入
for question in questions:
    print('=========\n你看过董付国老师的', question, '吗？')
    # 确保输入有效
    while True:
        print('没看过输入0，很多看不懂输入1，'
               '大部分可以看懂输入2，没压力输入3')
        try:
            answer = int(input('请输入：'))
            assert 0<=answer<=3
            break
        except:
            print('输入无效，请重新输入。')
            pass
    yourAnswer.append(answer)
    
print(clf.predict(np.array(yourAnswer).reshape(1,-1)))   # 分类
```

3. 完整代码

```python
from sklearn import tree
import numpy as np
# s数据
questions = ('《Python程序设计基础（第2版）》',
             '《Python程序设计基础与应用》',
             '《Python程序设计（第2版）》',
             '《大数据的Python基础》',
             '《Python程序设计开发宝典》',
             '《Python可以这样学》',
             '《中学生可以这样学Python》',
             '《Python编程基础与案例集锦（中学版）》',
             '《玩转Python轻松过二级》',
             '微信公众号“Python小屋”的免费资料',)
# 每个样本的数据含义：
# 0没看过，1很多看不懂，2大部分可以看懂，3没压力
answers = [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3],
           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
           [0, 0, 0, 0, 0, 0, 2, 2, 0, 1],
           [0, 0, 0, 0, 3, 3, 0, 0, 0, 3],
          
           [3, 3, 0, 3, 0, 0, 0, 0, 3, 1],
           [3, 0, 3, 0, 3, 0, 0, 3, 3, 2],
           [0, 0, 3, 0, 3, 3, 0, 0, 0, 3],
           [2, 2, 0, 2, 0, 0, 0, 0, 0, 1],
           [0, 2, 1, 3, 1, 1, 0, 0, 2, 1]
          ]

labels = ['超级高手', '门外汉', '初级选手', '初级选手', '高级选手',
          '中级选手', '高级选手', '超级高手', '初级选手', '初级选手']

clf = tree.DecisionTreeClassifier().fit(answers, labels) # 训练

yourAnswer = []
# 显示调查问卷，并接收用户输入
for question in questions:
    print('=========\n你看过董付国老师的', question, '吗？')
    # 确保输入有效
    while True:
        print('没看过输入0，很多看不懂输入1，'
               '大部分可以看懂输入2，没压力输入3')
        try:
            answer = int(input('请输入：'))
            assert 0<=answer<=3
            break
        except:
            print('输入无效，请重新输入。')
            pass
    yourAnswer.append(answer)
    
print(clf.predict(np.array(yourAnswer).reshape(1,-1)))   # 分类
```

**结果截图**

![image-20220318112038925](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318112038925.png)

## 支持向量机算法对手写数字图像进行分类

**程序思路**

1. 随程序生成1000张图片，并且保存在`D:\\workspaces\\AI\\ip\\人工智能\\数据集\\datasets`上，这些图片的数字生成是随机的`digit = choice(digits)`，最后把写到的数据都放在`digits.txt`文件中。

```python
# 生成图片
def generateDigits(dstDir='D:\\workspaces\\AI\\ip\\人工智能\\数据集\\datasets', num=1000):
    # 生成num个包含数字的图片文件存放于datasets子目录
    if not isdir(dstDir):
        mkdir(dstDir)
        
    # digits.txt用来存储每个图片对应的数字
    with open(dstDir+'\\digits.txt', 'w') as fp:
        for i in range(num):
            # 随机选择一个数字，生成对应的彩色图像文件
            digit = choice(digits)
            im = Image.new('RGB', (width,height), (255,255,255))
            imDraw = ImageDraw.Draw(im)
            font = truetype('c:\\windows\\fonts\\TIMESBD.TTF', fontSize)
            # 写入黑色数字
            imDraw.text((0,0), digit, font=font, fill=(0,0,0))
            # 加入随机干扰
            for j in range(int(noiseRate*width*height)):
                w, h = randrange(1, width-1), randrange(height)
                # 水平交换两个相邻像素的颜色
                c1 = im.getpixel((w,h))
                c2 = im.getpixel((w+1,h))                
                imDraw.point((w,h), fill=c2)
                imDraw.point((w+1,h), fill=c1)
            im.save(dstDir+'\\'+str(i)+'.jpg')
            fp.write(digit+'\n')
```

![image-20220318152713734](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318152713734.png)

2. 加载图片

```python
def loadDigits(dstDir='D:\\workspaces\\AI\\ip\\人工智能\\数据集\\datasets'):
    # 获取所有图像文件名
    digitsFile = [dstDir+'\\'+fn for fn in listdir(dstDir) if fn.endswith('.jpg')]
    # 按编号排序
    digitsFile.sort(key=lambda fn: int(basename(fn)[:-4]))
    # digitsData用于存放读取的图片中数字信息
    # 每个图片中所有像素值存放于digitsData中的一行数据
    digitsData = []
    for fn in digitsFile:
        with Image.open(fn) as im:
            # getpixel()方法用来读取指定位置像素的颜色值
            data = [sum(im.getpixel((w,h)))/len(im.getpixel((w,h)))
                    for w in range(width)
                    for h in range(height)]
            digitsData.append(data)
            
    # digitsLabel用于存放图片中数字的标准分类
    with open(dstDir+'\\digits.txt') as fp:
        digitsLabel = fp.readlines()
    # 删除数字字符两侧的空白字符
    digitsLabel = [label.strip() for label in digitsLabel]
    return (digitsData, digitsLabel)
```

3. 全部代码展示

```python
from os import mkdir,listdir
from os.path import isdir,basename
from random import choice,randrange
from string import digits
from PIL import Image,ImageDraw
from PIL.ImageFont import truetype
from sklearn import svm
from sklearn.model_selection import train_test_split

# 图像尺寸、图片中的数字字体大小、噪点比例
width, height = 30, 60
fontSize = 40
noiseRate = 8

# 生成图片
def generateDigits(dstDir='D:\\workspaces\\AI\\ip\\人工智能\\数据集\\datasets', num=1000):
    # 生成num个包含数字的图片文件存放于datasets子目录
    if not isdir(dstDir):
        mkdir(dstDir)
        
    # digits.txt用来存储每个图片对应的数字
    with open(dstDir+'\\digits.txt', 'w') as fp:
        for i in range(num):
            # 随机选择一个数字，生成对应的彩色图像文件
            digit = choice(digits)
            im = Image.new('RGB', (width,height), (255,255,255))
            imDraw = ImageDraw.Draw(im)
            font = truetype('c:\\windows\\fonts\\TIMESBD.TTF', fontSize)
            # 写入黑色数字
            imDraw.text((0,0), digit, font=font, fill=(0,0,0))
            # 加入随机干扰
            for j in range(int(noiseRate*width*height)):
                w, h = randrange(1, width-1), randrange(height)
                # 水平交换两个相邻像素的颜色
                c1 = im.getpixel((w,h))
                c2 = im.getpixel((w+1,h))                
                imDraw.point((w,h), fill=c2)
                imDraw.point((w+1,h), fill=c1)
            im.save(dstDir+'\\'+str(i)+'.jpg')
            fp.write(digit+'\n')

# 加载图片
def loadDigits(dstDir='D:\\workspaces\\AI\\ip\\人工智能\\数据集\\datasets'):
    # 获取所有图像文件名
    digitsFile = [dstDir+'\\'+fn for fn in listdir(dstDir) if fn.endswith('.jpg')]
    # 按编号排序
    digitsFile.sort(key=lambda fn: int(basename(fn)[:-4]))
    # digitsData用于存放读取的图片中数字信息
    # 每个图片中所有像素值存放于digitsData中的一行数据
    digitsData = []
    for fn in digitsFile:
        with Image.open(fn) as im:
            # getpixel()方法用来读取指定位置像素的颜色值
            data = [sum(im.getpixel((w,h)))/len(im.getpixel((w,h)))
                    for w in range(width)
                    for h in range(height)]
            digitsData.append(data)
            
    # digitsLabel用于存放图片中数字的标准分类
    with open(dstDir+'\\digits.txt') as fp:
        digitsLabel = fp.readlines()
    # 删除数字字符两侧的空白字符
    digitsLabel = [label.strip() for label in digitsLabel]
    return (digitsData, digitsLabel)

# 生成图片文件
generateDigits(num=1000)
# 加载数据
data = loadDigits()
print('数据加载完成。')

# 随机划分训练集和测试集，其中参数test_size用来指定测试集大小
X_train, X_test, y_train, y_test = train_test_split(data[0], data[1], test_size=0.1)
# 创建并训练模型
svcClassifier = svm.SVC(kernel="linear", C=1000, gamma=0.001)
svcClassifier.fit(X_train, y_train)
print('模型训练完成。')

# 使用测试集对模型进行评分
score = svcClassifier.score(X_test, y_test)
print('模型测试得分：', score)
```

**结果截图**

![image-20220318160124283](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318160124283.png)

## KNN算法判断交通工具

- 判断交通工具的标准在于时速，这里使用`unKnown=np.random.randint(0,1000,size=(20,4))`随机生成20个测试数据，数据范围是0-1000，以此来测试这个模型。

```python
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# X中存储交通工具的参数
# 总长度（米）、时速（km/h）、重量（吨）、座位数量
X = [[96, 85, 120, 400],        # 普通火车
      [144, 92, 200, 600],
      [240, 87, 350, 1000],
      [360, 90, 495, 1300],
      [384, 91, 530, 1405],
      [240, 360, 490, 800],      # 高铁
      [360, 380, 750, 1200],
      [290, 380, 480, 960],
      [120, 320, 160, 400],
      [384, 340, 520, 1280],
      [33.4, 918, 77, 180],      # 飞机
      [33.6, 1120, 170.5, 185],
      [39.5, 785, 230, 240],
      [33.84, 940, 150, 195],
      [44.5, 920, 275, 275],
      [75.3, 1050, 575, 490]]
# y中存储类别，0表示普通火车，1表示高铁，2表示飞机
y = [0]*5+[1]*5+[2]*6
# labels中存储对应的交通工具名称
labels = ('普通火车', '高铁', '飞机')

# 创建并训练模型
knn = KNeighborsClassifier(n_neighbors=3, weights='distance')
knn.fit(X, y)

# 对未知样本进行分类，随机生成数据
# unKnown = [[300, 79, 320, 900], [36.7, 800, 190, 220]]
unKnown=np.random.randint(0,1000,size=(20,4))
result = knn.predict(unKnown)
for para, index in zip(unKnown, result):
    print(para, labels[index], sep=':')

```

**结果截图**

![image-20220318213329954](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318213329954.png)

## KMeans聚类算法压缩图像颜色

- KMeans是一种无监督学习算法，在本实验中，选取四个样本空间为初始中心，不断比较附近的像素，更新聚类中心的像素值，直到最后无法更新，程序停止。

- 这里使用的是绝对路径`D:\\workspaces\\AI\\ip\\人工智能\\数据集\\...`
- 代码展示：

```python
import numpy as np
from sklearn.cluster import KMeans
from PIL import Image
import matplotlib.pyplot as plt

# 打开并读取原始图像中像素颜色值，转换为三维数组
imOrigin = Image.open('D:\\workspaces\\AI\\ip\\人工智能\\数据集\\颜色压缩测试图像.jpg')
dataOrigin = np.array(imOrigin)
# 然后再转换为二维数组，-1表示自动计算该维度的大小
data = dataOrigin.reshape(-1,3)

# 使用KMeans算法把所有像素的颜色值划分为4类
kmeansPredicter = KMeans(n_clusters=4)
kmeansPredicter.fit(data)

# 使用每个像素所属类的中心值替换该像素的颜色
# temp中存放每个数据所属类的标签
temp = kmeansPredicter.labels_
dataNew = kmeansPredicter.cluster_centers_[temp]
dataNew.shape = dataOrigin.shape
dataNew = np.uint8(dataNew)
plt.imshow(dataNew)
plt.imsave('D:\\workspaces\\AI\\ip\\人工智能\\数据集\\结果图像.jpg', dataNew)
plt.show()
```

**结果展示**

![image-20220318215650140](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318215650140.png)

![image-20220318215714548](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318215714548.png)

## 分层聚类算法验证实验

- 分层聚类又称为系统聚类算法或者系谱聚类算法，这个方法把所有样本都看做各自一类，定义类间距计算方式，选择距离最小的一对元素合并成一个新的类，重新计算各个类之间的距离并重复上面的步骤，直到所有原始元素划分为指定数量的类。
- 这个类计算的复杂度非常高，不适合大数据聚类问题
- 代码展示：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering

def AgglomerativeTest(n_clusters):
    assert 1 <= n_clusters <= 4
    predictResult = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean',
                                            linkage='ward').fit_predict(data)
    # 定义绘制散点图时使用的颜色和散点符号
    colors = 'rgby'
    markers = 'o*v+'
    # 依次使用不同的颜色和符号绘制每个类的散点图
    for i in range(n_clusters):
        subData = data[predictResult==i]
        plt.scatter(subData[:,0], subData[:,1], c=colors[i], marker=markers[i], s=40)
    plt.show()


# 生成随机数据，200个点，分成4类，返回样本及标签
data, labels = make_blobs(n_samples=200, centers=4)
print(data)
AgglomerativeTest(3)
AgglomerativeTest(4)
```

**结果截图**

![image-20220318221811818](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318221811818.png)

![image-20220318221841325](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318221841325.png)

## DBSCAN算法验证实验

- 属于密度聚类算法，把类定义为密度相连对象的最大集合，把类定义为密度相连对象的最大集合，通过在样本空间中不断搜索高密度的核心样本并扩展得到最大集合完成聚类，能够在带有噪点的样本空间中发现任意形状的聚类并排除噪点。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_blobs

def DBSCANtest(data, eps=0.6, min_samples=8):
    # 聚类
    db = DBSCAN(eps=eps, min_samples=min_samples).fit(data)
        
    # 聚类标签（数组，表示每个样本所属聚类）和所有聚类的数量
    # 标签-1对应的样本表示噪点
    clusterLabels = db.labels_
    uniqueClusterLabels = set(clusterLabels)
    # 标记核心对象对应下标为True
    coreSamplesMask = np.zeros_like(db.labels_, dtype=bool)
    coreSamplesMask[db.core_sample_indices_] = True

    # 绘制聚类结果
    colors = ['red', 'green', 'blue', 'gray', '#88ff66',
              '#ff00ff', '#ffff00', '#8888ff', 'black',]
    markers = ['v', '^', 'o', '*', 'h', 'd', 'D', '>', 'x']
    for label in uniqueClusterLabels:
        # 使用最后一种颜色和符号绘制噪声样本
        # clusterIndex是个True/False数组
        # 其中True表示对应样本为cluster类
        clusterIndex = (clusterLabels==label)
        
        # 绘制核心对象
        coreSamples = data[clusterIndex&coreSamplesMask]
        plt.scatter(coreSamples[:, 0], coreSamples[:, 1],
                    c=colors[label], marker=markers[label], s=100)

        # 绘制非核心对象
        nonCoreSamples = data[clusterIndex & ~coreSamplesMask]
        plt.scatter(nonCoreSamples[:, 0], nonCoreSamples[:, 1],
                    c=colors[label], marker=markers[label], s=20)
    plt.show()

data, labels = make_blobs(n_samples=300, centers=5)
DBSCANtest(data)
DBSCANtest(data, 0.8, 15)

```

![image-20220318224228052](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318224228052.png)

## 协同过滤算法

- 这个算法适合于商品推荐、商品捆绑。根据用户之间或商品之间的相似性进行精准推荐，可以分为基于用户的协同过滤算法和基于商品的协同过滤算法。

```python
from random import randrange

# 模拟历史电影打分数据，共10个用户，每个用户打分的电影数量不等
data = {'user'+str(i):{'film'+str(randrange(1, 15)):randrange(1, 6) for j in range(randrange(3, 10))}
         for i in range(10)}
# 寻求推荐的用户对电影打分的数据
user = {'film'+str(randrange(1, 15)):randrange(1,6) for i in range(5)}

# 最相似的用户及其对电影打分情况
# 两个最相似的用户共同打分的电影最多，同时所有电影打分差值的平方和最小
rule = lambda item:(-len(item[1].keys()&user),
                    sum(((item[1].get(film)-user.get(film))**2 for film in user.keys()&item[1].keys())))
similarUser, films = min(data.items(), key=rule)
# 输出信息以便验证，每行数据有3列
# 分别为该用户与当前用户共同打分的电影数量、打分差的平方和、该用户打分数据
print('known data'.center(50, '='))
for item in data.items():
    print(len(item[1].keys()&user.keys()),
          sum(((item[1].get(film)-user.get(film))**2 for film in user.keys()&item[1].keys())),
          item, sep=':')
print('current user'.center(50, '='), user, sep='\n')
print('most similar user and his films'.center(50, '='))
print(similarUser, films, sep=':')
print('recommended film'.center(50, '='))
# 在当前用户没看过的电影中选择打分最高的进行推荐
print(max(films.keys()-user.keys(), key=lambda film: films[film]))

```

**结果截图**

![image-20220318225908714](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220318225908714.png)

## 使用关联规则分析演员关系

```python
from itertools import chain, combinations
from openpyxl import load_workbook

def loadDataSet():
    '''加载数据，返回包含若干集合的列表'''
    # 返回的数据格式为 [{1, 3, 4}, {2, 3, 5}, {1, 2, 3, 5}, {2, 5}]
    result = []
    # xlsx文件中有3列，分别为电影名称、导演名称、演员清单
    # 同一个电影的多个主演演员使用逗号分隔
    ws = load_workbook('D:\\workspaces\\AI\\ip\\人工智能\\数据集\\电影导演演员.xlsx').worksheets[0]
    for index, row in enumerate(ws.rows):
        # 跳过第一行表头
        if index==0:
            continue
        result.append(set(row[2].value.split('，')))
    return result

def createC1(dataSet):
    '''dataSet为包含集合的列表，每个集合表示一个项集
       返回包含若干元组的列表，
       每个元组为只包含一个物品的项集，所有项集不重复'''
    return sorted(map(lambda i:(i,), set(chain(*dataSet))))

def scanD(dataSet, Ck, Lk, minSupport):
    '''dataSet为包含集合的列表，每个集合表示一个项集
       ck为候选项集列表，每个元素为元组
       minSupport为最小支持度阈值
       返回Ck中支持度大于等于minSupport的那些项集'''
    # 数据集总数量
    total = len(dataSet)
    supportData = {}
    for candidate in Ck:
        # 加速，k-频繁项集的所有k-1子集都应该是频繁项集
        if Lk and (not all(map(lambda item: item in Lk,
                                 combinations(candidate,
                                               len(candidate)-1)))):
            continue
        # 遍历每个候选项集，统计该项集在所有数据集中出现的次数
        # 这里隐含了一个技巧：True在内部存储为1
        set_candidate = set(candidate)
        frequencies = sum(map(lambda item: set_candidate<=item,
                                dataSet))
        # 计算支持度
        t = frequencies/total
        # 大于等于最小支持度，保留该项集及其支持度
        if t >= minSupport:
            supportData[candidate] = t
    return supportData

def aprioriGen(Lk, k):
    '''根据k项集生成k+1项集'''
    result = []
    for index, item1 in enumerate(Lk):
        for item2 in Lk[index+1:]:
            # 只合并前k-2项相同的项集，避免生成重复项集
            # 例如，(1,3)和(2,5)不会合并，
            # (2,3)和(2,5)会合并为(2,3,5)，
            # (2,3)和(3,5)不会合并，
            # (2,3)、(2,5)、(3,5)只能得到一个项集(2,3,5)
            if sorted(item1[:k-2]) == sorted(item2[:k-2]):
                result.append(tuple(set(item1)|set(item2)))
    return result

def apriori(dataSet, minSupport=0.5):
    '''根据给定数据集dataSet，
       返回所有支持度>=minSupport的频繁项集'''
    C1 = createC1(dataSet)
    supportData = scanD(dataSet, C1, None, minSupport)
    k = 2
    while True:
        # 获取满足最小支持度的k项集
        Lk = [key for key in supportData if len(key)==k-1]
        # 合并生成k+1项集
        Ck = aprioriGen(Lk, k)
        # 筛选满足最小支持度的k+1项集
        supK = scanD(dataSet, Ck, Lk, minSupport)
        # 无法再生成包含更多项的项集，算法结束
        if not supK:
            break
        supportData.update(supK)
        k = k+1
    return supportData

def findRules(supportData, minConfidence=0.5):
    '''查找满足最小置信度的关联规则'''
    # 对频繁项集按长度降序排列
    supportDataL = sorted(supportData.items(),
                            key=lambda item:len(item[0]),
                            reverse=True)
    rules = []
    for index, pre in enumerate(supportDataL):
        for aft in supportDataL[index+1:]:
            # 只查找k-1项集到k项集的关联规则
            if len(aft[0]) < len(pre[0])-1:
                break
            # 当前项集aft[0]是pre[0]的子集
            # 且aft[0]==>pre[0]的置信度大于等于最小置信度阈值
            if set(aft[0])<set(pre[0]) and\
               pre[1]/aft[1]>=minConfidence:
                rules.append([pre[0],aft[0]])
    return rules

# 加载数据
dataSet = loadDataSet()
# 获取所有支持度大于0.2的项集
supportData = apriori(dataSet, 0.2)
# 在所有频繁项集中查找并输出关系较好的演员二人组合
bestPair = [item for item in supportData if len(item)==2]
print(bestPair)

# 查找支持度大于0.6的强关联规则
for item in findRules(supportData, 0.6):
    pre, aft = map(set, item)
    print(aft, pre-aft, sep='==>')

```

**结果截图：**

![image-20220319132912257](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220319132912257.png)

## 使用交叉验证与网格搜索，进行支持向量机分类模型的参数学习

- 交叉验证会反复对数据集进行划分，并使用不同的划分对模型进行评分，可以更好地评估模型的泛化质量。

```python
from time import time
from os import listdir
from os.path import basename
from PIL import Image
from sklearn import svm
from sklearn.model_selection import cross_val_score,\
     ShuffleSplit, LeaveOneOut

# 图像尺寸
width, height = 30, 60

def loadDigits(dstDir='D:\\workspaces\\AI\\ip\\人工智能\\数据集\\datasets'):
    # 获取所有图像文件名
    digitsFile = [dstDir+'\\'+fn for fn in listdir(dstDir)
                  if fn.endswith('.jpg')]
    # 按编号排序
    digitsFile.sort(key=lambda fn: int(basename(fn)[:-4]))
    # digitsData用于存放读取的图片中数字信息
    # 每个图片中所有像素值存放于digitsData中的一行数据
    digitsData = []
    for fn in digitsFile:
        with Image.open(fn) as im:
            data = [sum(im.getpixel((w,h)))/len(im.getpixel((w,h)))
                    for w in range(width)
                    for h in range(height)]
            digitsData.append(data)
    # digitsLabel用于存放图片中数字的标准分类
    with open(dstDir+'\\digits.txt') as fp:
        digitsLabel = fp.readlines()
    digitsLabel = [label.strip() for label in digitsLabel]
    return (digitsData, digitsLabel)

# 加载数据
data = loadDigits()
print('数据加载完成。')

# 创建模型
svcClassifier = svm.SVC(kernel="linear", C=1000, gamma=0.001)

# 交叉验证
start = time()
scores = cross_val_score(svcClassifier, data[0], data[1], cv=8)
print('交叉验证（k折叠）得分情况：\n', scores)
print('平均分：\n', scores.mean())
print('用时（秒）：', time()-start)
print('='*20)

start = time()
scores = cross_val_score(svcClassifier, data[0], data[1],
                         cv=ShuffleSplit(test_size=0.3,
                                         train_size=0.7,
                                         n_splits=10))
print('交叉验证（随机拆分）得分情况：\n', scores)
print('平均分：\n', scores.mean())
print('用时（秒）：', time()-start)
print('='*20)

start = time()
scores = cross_val_score(svcClassifier, data[0], data[1],
                         cv=LeaveOneOut())
print('交叉验证（逐个测试）得分情况：\n', scores)
print('平均分：\n', scores.mean())
print('用时（秒）：', time()-start)
```

**结果展示**

![image-20220319150359392](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220319150359392.png)

## 对我国人工智能或机器学习应用研究的现状与趋势介绍，以及对自己将来职业规划的感想等

当前人工智能正在迅猛发展，主要表现在人工智能产业规模稳步上升，数字化经济不断发展，国家和地方政府顺应时代要求，出台了很多有利于人工智能发展的帮扶政策，在这样的大背景下，市场对人工智能行业的投资热情不断上涨，相关的技术也在不断的成熟。虽然我国的人工智能产业起步比较晚，但是顺应这历史的发展，也取到了不俗的成就。经过中国信息通信研究院的测算，我国2021的AI产业规模达到434亿美元，同比增长15%。其中，人工智能最被关注的点在于它促进了传统产业的转型升级。由此产生很多关于它的应用，比如“AI+交通”，“AI+医疗”，“AI+零售”等。以交通为例，在交通工具上，AI通过深度学习，学习交通规则、利用计算机视觉和人工大脑，快速识别路面情况并且做出正确的选择；在出行导航上，AI可以实时识别道路情况、掌握分析并计算最佳道路、反馈道路信息（如堵车、封路等），这不仅仅能合理利用交通资源，还能减少事故发生率。当然，诸如“AI+交通”这样的例子还有很多。但是不可避免的是，如今的AI产业虽然有所作为，但是成熟的应用还是十分欠缺的，但是正是这一不成熟，给人工智能产业带来无数的机遇。在未来，人工智能算法算力的深入、人工智能神经网络发展、人工智能伦理问题等，都将是需要解决而必须解决的问题。

对于处于这样一个机遇与挑战并存的时代的当代大学生，在就业方面难免会有相应的压力，毕竟需要大多数的体力活都被或者将被机器和算法替代，因此作为当代大学生，更应该在大学期间，以能力培育为基础，需要学会的知识，要学精，并且要树立终身学习的目标。

# 第二次实验报告目录

> 实验名称：

中文文本分词、词性标注与命名实体识别算法综合实验

> 实验内容如下：

1. 实验数据：200篇新闻

2. 实验步骤：

（1）预处理：将标题和正文中的所有可能包含的html格式符号清除干净，并按照句号和问号进行分行显示（每一行为一句话）

（2）分词与词性标注实验：

① 使用逆向最大匹配分词算法、[HanLP](https://www.jianshu.com/p/4870fbe89d32)以及jieba分词的精确分词模式对预处理之后的实验数据200篇新闻文本进行[分词与词性标注](https://www.jianshu.com/p/5f5c30d1e225)以及jieba分词的精确分词模式对预处理之后的实验数据200篇新闻文本进行分词与词性标注；

② 并选取***2***篇新闻【按照学号来进行选取，比如学号***1***结尾的同学，选取文件名字以***1***结尾的两篇新闻文本；比如学号***2***结尾的同学，选取文件名字以***2***结尾的两篇新闻文本，等等】，人工标注分词结果，只需要标注分词，不需要标注词性；

③ 然后对不同算法的分词结果进行性能比较（正确分词的词数目/这2篇新闻的人工标注分词的词总数），并进行错误分析（将人工标注结果与分词错误结果列在一张表格中，从而对比出这几个分词算法各自或者共同的分词错误样例）。

（3）命名实体识别实验：

① 用jieba分词工具，根据词性获取实体对象，从而完成命名实体识别（人名、地名、机构名）实验，【https://blog.csdn.net/fgg1234567890/article/details/115315068】

nr	人名	名词代码n和“人(ren)”的声母并在一起

ns	地名	名词代码n和处所词代码s并在一起

nt	机构团体	“团”的声母为t，名词代码n和t并在一起

nz	其他专名	“专”的声母的第1个字母为z，名词代码n和z并在一起

② 并人工标注分词实验中的人工标注的那2篇新闻的命名实体识别的结果，计算命名实体识别准确率（正确命名实体数目/这2篇新闻的人工标注命名实体总数），进一步做错误分析

③ 【附加题】用课件中HMM算法【可参考：https://www.cnblogs.com/lokvahkoor/p/12781056.html】来进行命名实体识别的模型训练与测试，【训练数据可以用：实验数据/第5章/1980_01rmrb.txt，已上传在005-006-实验代码参考与数据集.zip】并对比出jieba与HMM模型各自或者共同	的实体识别错误样例；

##  (1)预处理

> 要求：将标题和正文中的所有可能包含的html格式符号清除干净，并按照句号和问号进行分行显示（每一行为一句话）

1. 首先打开获取到文本的路径，因为文本的命名不是连续的，所以用的是`os.listdir( path )`来获取到路径的链表：

   ```python
   # 目录
   path = "D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\第二次实验-分词实验数据"
   # 获取目录下所有的文件的名字
   dirs = os.listdir( path )
   ```

2. 获得到的路径，打开文本对文本去除html格式和空行，这里用的是字符串处理的`replace()`函数

   ```python
   line=line.replace('&nbsp','').replace('&lt','').replace('&gt','').replace('&amp','').replace('&quot','').replace('&copy','').replace('&reg','')
   ```

3. 去除html和空行之后，使用正则表达式对文本切割，如：

   ```python
   line_write=re.split(r'。|？',line)
   ```

4. 切割文本后，装载到一个列表中，并把收集到的数据逐行写到新的文本中去

   ```python
   # 写数据
   with open(aft_path,'w',encoding='gb18030') as txt:
       print(f'打开{aft_path}文件')
       for line in file_string:
           txt.write(line+'\n')
           print(f'打开{aft_path}文件')
           txt.close()
   ```

5. 以上的全部代码总和：

   ```python
   import os
   import re
   
   def txt_extraction(bef_path,aft_path):
       """提取文本，并把提取到的文本写到新的文件中"""
       file_string=[]
       # 打开文件
       with open(bef_path,encoding='gb18030') as txt:
           print(f'打开{bef_path}文件')
           for line in txt:
               # 去除段落和空格换行
               line=line.strip()
               # 去除空行
               if line=='':
                   continue
               # 查找html格式内容，并去除
               line=line.replace('&nbsp','').replace('&lt','').replace('&gt','').replace('&amp','').replace('&quot','').replace('&copy','').replace('&reg','')
               # 按照句号或者问号切割
               line_write=re.split(r'。|？',line)
               for i in line_write:
                   # 获取到全部内容并保存
                   file_string.append(i)
   #         print(file_string)
           print(f'关闭{bef_path}文件')
           txt.close()
   
   #     # 文件不存在，新建文件
   #     if not os.path.exists(aft_path):
   #         os.makedirs(aft_path)
   #         print('目录创建成功')
   
       # 写数据
       with open(aft_path,'w',encoding='gb18030') as txt:
           print(f'打开{aft_path}文件')
           for line in file_string:
               txt.write(line+'\n')
           print(f'打开{aft_path}文件')
           txt.close()
   
   
   def main():
       """主函数"""
       # 目录
       path = "D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\第二次实验-分词实验数据"
       # 获取目录下所有的文件的名字
       dirs = os.listdir( path )
   
       for file in dirs:
           bef_path=path+'\\'+file
           aft_path='D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\\\分词实验预处理后数据\\'+file
           txt_extraction(bef_path,aft_path)
   
   main()
   ```

6. 输出结果：在文件处理的时候，我对于打开文件和关闭文件进行了输出路径的操作

<img src="%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220402212331640.png" alt="image-20220402212331640" style="zoom:67%;" />

7. 文本处理前后对比

   <img src="%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220402212539952.png" alt="image-20220402212539952" style="zoom:67%;" />

   <img src="%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220402212514979.png" alt="image-20220402212514979" style="zoom:67%;" />

## (2)分词与词性标注实验

> 要求：使用逆向最大匹配分词算法、[HanLP](https://www.jianshu.com/p/5f5c30d1e225)以及jieba分词的精确分词模式对预处理之后的实验数据200篇新闻文本进行[分词与词性标注](https://www.jianshu.com/p/4870fbe89d32)；
>

### 逆向最大匹配算法

这里是参考实例中的逆向最大匹配算法修改后的代码，对于单独成词的数据，这里同样是收录在结果中的。

1. 逆向最大匹配分词算法对预处理后的实验数据100篇新闻文本进行分词

2. 第一步先加载预处理后的实验数据：`dirs = os.listdir(path)`获取路径下的文件，以`encoding='gb18030'`格式打开，代码如下：

   ```python
   def main():
       path = "D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\分词实验预处理后数据\\"
       dirs = os.listdir(path)
       for file in dirs:
           with open(path+file,encoding='gb18030') as txt:
               text=txt.read()
               tokenizer = IMM('D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\实验分词所用词典-dict.txt.big\\dict.txt.big')
               print(file,end=':')
               print(tokenizer.cut(text)[0:10])
   ```

3. 第二步是加载词典中的内容，去掉空行后，存储在`set()`中，并记录其中最长的词的长度，代码如下：

   ```python
   def __init__(self, dic_path):
       """
       获取到dic_path中的词典的内容，并存在dictionary元素集中
       同步记录字典的最大行数，存储在maximum中
       """
       # 创建一个无序不重复元素
       self.dictionary = set()
       self.maximum = 0
       #读取词典
       with open(dic_path, 'r', encoding='utf8') as f:
           for line in f:
               # 切割并去掉空行
               line = line.strip()
               if not line:
                   continue
                   self.dictionary.add(line)
                   # 记录最大行数
                   if len(line) > self.maximum:
                       self.maximum = len(line)
   ```

4. 第三步剪切数据，首先从文本最后取`maximum`长度的数据，如果词典有这个数据，就剪切，否则减掉数据前面的一个字，重复上面操作，直到数据长度为1，如果还是匹配不到，就记为单个字，记录在结果中，否则就剪切。代码如下：

   ```python
       def cut(self, text):
           """
           按照字典中的数据对文本进行切割，如果字典中没有对应的数据就单独成字
           """
           result = []
           # 获取到全部的text的内容，只要内容的长度不为0，就一直在搜寻
           index = len(text)
           while index > 0:
               word = None
               # 获取后面maximum数据，一直到匹配后才结束size[maximum,maximum-1……1]
               for size in range(self.maximum, 0, -1):
                   if index - size < 0:
                       continue
                   # 获取到后面maximum的数据碎片
                   piece = text[(index - size):index]
                   # 字典可以找到数据，进入if
                   if piece in self.dictionary:
                       word = piece
                       result.append(word)
                       index -= size
                       break
                   # 字典找不到数据，说明是单独成字，一样要合并收集
                   if len(piece)==1:
                       # 不要空行，也不要标点符号
                       if piece==' ':
                           continue
                       result.append(piece)
               if word is None:
                   index -= 1
           return result[::-1]
   ```

5. 关于输出，因为直接打印出来的数据过分多，所以在屏幕下打印每一份文本的前10个数据，代码如下：

   ```PYTHON
   tokenizer = IMM('D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\实验分词所用词典-dict.txt.big\\dict.txt.big')
   print(file,end=':')
   print(tokenizer.cut(text)[0:10])
   ```

6. 全部代码如下：

   ```python
   import os
   #逆向最大匹配
   class IMM(object):
       def __init__(self, dic_path):
           """
           获取到dic_path中的词典的内容，并存在dictionary元素集中
           同步记录字典的最大行数，存储在maximum中
           """
           # 创建一个无序不重复元素
           self.dictionary = set()
           self.maximum = 0
           #读取词典
           with open(dic_path, 'r', encoding='utf8') as f:
               for line in f:
                   # 切割并去掉空行
                   line = line.strip()
                   if not line:
                       continue
                   self.dictionary.add(line)
                   # 记录最大行数
                   if len(line) > self.maximum:
                       self.maximum = len(line)
       def cut(self, text):
           """
           按照字典中的数据对文本进行切割，如果字典中没有对应的数据就单独成字
           """
           result = []
           # 获取到全部的text的内容，只要内容的长度不为0，就一直在搜寻
           index = len(text)
           while index > 0:
               word = None
               # 获取后面maximum数据，一直到匹配后才结束size[maximum,maximum-1……1]
               for size in range(self.maximum, 0, -1):
                   if index - size < 0:
                       continue
                   # 获取到后面maximum的数据碎片
                   piece = text[(index - size):index]
                   # 字典可以找到数据，进入if
                   if piece in self.dictionary:
                       word = piece
                       result.append(word)
                       index -= size
                       break
                   # 字典找不到数据，说明是单独成字，一样要合并收集
                   if len(piece)==1:
                       # 不要空行，也不要标点符号
                       if piece==' ':
                           continue
                       result.append(piece)
               if word is None:
                   index -= 1
           return result[::-1]
   
   def main():
       path = "D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\分词实验预处理后数据\\"
       dirs = os.listdir(path)
       for file in dirs:
           with open(path+file,encoding='gb18030') as txt:
               text=txt.read()
               tokenizer = IMM('D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\实验分词所用词典-dict.txt.big\\dict.txt.big')
               print(file,end=':')
               print(tokenizer.cut(text)[0:10])
   
   main()
   ```

7. 输出截图：

   这里因为文本编码问题一直都是报错，所以最后改成了`gb18030`格式的文本打开，但是这样的结果就是智能单个字的切割数据
   
   <img src="%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220403211345666.png" alt="image-20220403211345666" style="zoom:67%;" />

### HanLp分词和词性标注

1. 安装：注意的是，这个需要以来java的jdk环境，如果没有安装jdk会出现报错

   ```base
   conda activate test
   pip install pyhanlp
   ```

2. 以命令行形式进行分词交互

   ```base
   hanlp segment
   ```

   结果截图：

   ![image-20220404163315394](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220404163315394.png)

   可以看到运行一切正常。

3. 在anaconda中使用HanLp分词和词性标注

   ```python
   import os
   from pyhanlp import *
   
   
   path="D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\分词实验预处理后数据\\"
   dirs=os.listdir(path)
   for file in dirs:
       with open(path+file,encoding='ANSI') as txt:
           text = txt.read()
   
           print(file)
           print(HanLP.segment(text)[0:10])
   ```

4. 结果截图：

   ![image-20220404163510770](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220404163510770.png)



### jieba精准模式分词和词性标注

1. jiaba常用的模式有三种：

   - 精确模式，试图将句子最精确地切开，适合文本分析；

   - 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；

   - 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

   - **一般默认是精确模式，`cut_all=False`是精确模式，`cut_all=True`是全模式，`cut_for_search`是搜索引擎模式**。
   - **jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型。**
   - **jieba.cut_for_search方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细。**

2. 用`lcut`生成list：

   jieba.cut 以及jieba.cut_for_search返回的结构都是一个可迭代的 Generator，可以使用 for 循环来获得分词后得到的每一个词语（Unicode）。jieba.lcut 对 cut 的结果做了封装，l 代表 list，即返回的结果是一个 list 集合。同样的，用jieba.lcut_for_search也直接返回 list 集合。

3. 获取词性

   jieba 可以很方便地获取中文词性，通过 jieba.posseg 模块实现词性标注。

4. 代码：

   ```python
   import os
   # 引入词性标注接口
   import jieba.posseg as psg
   
   
   path="D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\分词实验预处理后数据\\"
   dirs=os.listdir(path)
   for file in dirs:
       with open(path+file,encoding='ANSI') as txt:
           text = txt.read()
           # 精准分词并获取词性,这里输出是文件名和切割后的数据以及词性，因为篇幅问题，这里只展示0-10的词语
           print(file)
           print([(x.word,x.flag) for x in psg.lcut(text)[0:10]])
   ```

5. 输出截图

   ![image-20220404162825833](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220404162825833.png)

### 人工分词

> 要求：并选取***2***篇新闻【按照学号来进行选取，比如学号***1***结尾的同学，选取文件名字以***1***结尾的两篇新闻文本；比如学号***2***结尾的同学，选取文件名字以***2***结尾的两篇新闻文本，等等】，人工标注分词结果，只需要标注分词，不需要标注词性；

说明：学号是`1`号，这里选取的是`121.txt`和`171.txt`两篇文章，因为人工分词的工作量有点大，所以这里使用`snownlp`分词工具来代替，具体实现逻辑是：对一整个文本数据进行按行切割，切割后的数据按照每一行一个列表的形式存储在`result`中，方便后面的使用，具体实现代码：

```python
# SnowNLP小用-用来表示人工切分数据
from snownlp import SnowNLP
def man_make_result(path):
    """
    模拟人工分词
    """
    result=[]


    with open(path,encoding='ansi') as txt:
        # 整篇文章
        text=txt.read()
        text_line=text.split()
        for line in text_line:
            l=SnowNLP(line)
            result.append(l.words)
#             print(l.words)
    return result
                
# man_make()
```



### 不同算法分词的性能比较

> 要求：然后对不同算法的分词结果进行性能比较（正确分词的词数目/这2篇新闻的人工标注分词的词总数），并进行错误分析（将人工标注结果与分词错误结果列在一张表格中，从而对比出这几个分词算法各自或者共同的分词错误样例）。
>

#### jieba分词与人工分词性能比较

说明：这里输出的顺序是按照每一行来输出，首先输出文件名和人工分词的一行，接着输出jieba分词的一行，如果两行相同输出yes，不同输出相对于人工分词jieba不同的数量以及jieba不同的具体内容（方便比较），最后所有行都已经输出完毕，直接输出jieba相对人工分词的错误率。实现如下：

1. 首先是获取到文件，因为这里只需要两个文件，所以这里定义了一个列表，把两个文件固定住，通过遍历列表的元素就可以获得对应路径；再者一个重要的点就是获取到上面的人工分词的结果集`result`(这里直接调用上面的函数就可以了`result=man_make_result(path+file)`)，作为下面调用函数的一个参数`jieba_result(result,path+file)`,其主函数实现如下：

   ```python
   def main_jieba():
       files=['121.txt','171.txt']
       path="D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\人工分词和分析\\"
   
       for file in files:
           print(file)
           # 人工分词
           result=man_make_result(path+file)
           # jieba分词
           jieba_result(result,path+file)
   ```

2. jieba分词的划分第一步就是先对文章进行切割，并且按照行的形式对切割后的结果进行保存，并且统计每一行的切割的数目`sum=sum+len(write_str)`实现如下：

   ```python
   # 获取一整篇文章，并且切割成一行，存储成list
   text=txt.read()
   text_line=text.split()
   
   # 对一行进行jieba的精准切割，切割后的分词存储在write_str
   for line in text_line:
       temp=psg.lcut(line)
       write_str=[]
       # 一整行全部保存在write_str中
       for i in temp:
           write_str.append(i.word)
           print('人工:',result[index])
           print('jieba:',write_str)
           sum=sum+len(write_str)
   ```

3. jieba分词划分的第二步是把上面得到的每一行的结果`write_str`人工分词的对应行的结果`result[index]`，判断相同就输出yes，不同就记录不同的数据到`temp`列表中，并且统计一行中的不同数据的数量`j`，最后得到j的累加`dif_sum`，具体实现如下：

   ```python
   # 比较一行与人工分词的区别
   if write_str==result[index]:
       print('yes')
       else:
           j=0
           temp=[]
           for i in write_str:
               if i not in result[index]:
                   j=j+1
                   temp.append(i)
                   print('jieba不同的数量:',j,' jieba不同的数据是：',temp)
            dif_sum=dif_sum+j
   ```

4. 输出打印错误率

   ```python
   # 输出错误率
   print(jieba_path[-7:-1],'的jieba错误率是',dif_sum/sum)
   ```

5. 全部的实现代码

   ```python
   import os
   import jieba.posseg as psg
   
   # jieba分词后结果
   def jieba_result(result,jieba_path):
       """
       jieba分词后结果
       index:表示人工分词result的每一行的标号
       sum:表示jieba分词的分词总数，是每一行切割后列表的长度的和
       dif_sum:表示jieba和人工的划分的不同的数量的和，在本代码中，是j的和
       """
       index=0
       sum=0
       dif_sum=0
       with open(jieba_path,encoding='ansi') as txt:
           # 获取一整篇文章，并且切割成一行，存储成list
           text=txt.read()
           text_line=text.split()
           
           # 对一行进行jieba的精准切割，切割后的分词存储在write_str
           for line in text_line:
               temp=psg.lcut(line)
               write_str=[]
               # 一整行全部保存在write_str中
               for i in temp:
                   write_str.append(i.word)
               print('人工:',result[index])
               print('jieba:',write_str)
               sum=sum+len(write_str)
               
               # 比较一行与人工分词的区别
               if write_str==result[index]:
                   print('yes')
               else:
                   j=0
                   temp=[]
                   for i in write_str:
                       if i not in result[index]:
                           j=j+1
                           temp.append(i)
                   print('jieba不同的数量:',j,' jieba不同的数据是：',temp)
                   dif_sum=dif_sum+j
               index+=1
       # 输出错误率
       print(jieba_path[-7:-1],'的jieba错误率是',dif_sum/sum)
       
   def main_jieba():
       files=['121.txt','171.txt']
       path="D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\人工分词和分析\\"
   
       for file in files:
           print(file)
           # 人工分词
           result=man_make_result(path+file)
           # jieba分词
           jieba_result(result,path+file)
           
   main_jieba()
   ```

6. 结果截图：截图太长，只截图一部分

   这里我们可以看到，相对于人工分词，`121.txt`结巴分词的错误率是`0.1750`，`171.txt`的错误率是`0.1133`

   ![image-20220404234340126](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220404234340126.png)

   ![image-20220404234527519](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220404234527519.png)

   ![image-20220404234554067](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220404234554067.png)

#### HanLP分词与人工分词性能比较

说明：这里输出的顺序是按照每一行来输出，首先输出文件名和人工分词的一行，接着输出HanPL分词的一行，如果两行相同输出yes，不同输出相对于人工分词jieba不同的数量以及jieba不同的具体内容，最后所有行都已经输出完毕，直接输出HanPL相对人工分词的错误率，并且在实现上，和上面jieba分词的算法逻辑是一样的，所以这里我也不重复逻辑了，直接放全部的代码：

```python
import os
from pyhanlp import *
# 去掉词性
HanLP.Config.ShowTermNature = False

# HanPL分词错误比较
def hanPL_result(result,hanpl_path):
    """
    HanPL分词错误比较（与人工）
    index:表示人工分词result的每一行的标号
    sum:表示jieba分词的分词总数，是每一行切割后列表的长度的和
    dif_sum:表示jieba和人工的划分的不同的数量的和，在本代码中，是j的和
    """
    index=0
    sum=0
    dif_sum=0
    with open(hanpl_path,encoding='ansi') as txt:
        # 获取一整篇文章，并且切割成一行，存储成list
        text=txt.read()
        text_line=text.split()
        
        # 对一行进行jieba的精准切割，切割后的分词存储在write_str
        for line in text_line:
            temp=HanLP.segment(line)
            write_str=[]
            for i in temp:
                write_str.append(i.word)
            
            print('人工:',result[index])
            print('HanPL:',write_str)
            sum=sum+len(write_str)
            
            if write_str==result[index]:
                print('yes')
            else:
                j=0
                temp=[]
                for i in write_str:
                    if i not in result[index]:
                        j=j+1
                        temp.append(i)
                print('HanPL不同的数量:',j,' HanPL不同的数据是：',temp)
                dif_sum=dif_sum+j
            # index一定要最后才加1，不然会出现顺序不一样
            index+=1
    # 输出错误率
    print(hanpl_path[-7:-1],'的HanPL错误率是',dif_sum/sum)      
            
def main_hanLP():
#     files=['121.txt','171.txt']
    files=['121.txt']
    path="D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\人工分词和分析\\"

    for file in files:
        print(file)
        # 人工分词
        result=man_make_result(path+file)
        # HanPL分词
        hanPL_result(result,path+file)
        
main_hanLP()
```

- 结果截图

  这里我们可以看到，相对于人工分词，`121.txt`HanLP分词的错误率是`0.1885`，`171.txt`的错误率是`0.1208`

![image-20220404235039569](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220404235039569.png)

![image-20220404234957282](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220404234957282.png)

![image-20220404235354969](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220404235354969.png)

## (3)命名实体识别实验

### jieba分词两种算法的命名实体识别

> 要求：用jieba分词工具，根据词性获取实体对象，从而完成[命名实体识别（人名、地名、机构名）实验](https://blog.csdn.net/fgg1234567890/article/details/115315068)
>

nr	人名	名词代码n和“人(ren)”的声母并在一起

ns	地名	名词代码n和处所词代码s并在一起

nt	机构团体	“团”的声母为t，名词代码n和t并在一起

nz	其他专名	“专”的声母的第1个字母为z，名词代码n和z并在一起



1. `jieba.analyse.extract_tags(sentence, topK=5, withWeight=True, allowPOS=())`

   - sentence 需要提取的字符串，必须是str类型，不能是list
   - topK 提取前多少个关键字
   - withWeight 是否返回每个关键词的权重
   - llowPOS是允许的提取的词性，默认为allowPOS=‘ns’, ‘n’, ‘vn’, ‘v’，提取地名、名词、动名词、动词

   - jieba.analyse.extract_tags()提取关键字的原理是使用TF-IDF算法

   ```python
   with open(path+file,encoding='ansi') as txt:
           text=txt.read()
           kw = jieba.analyse.extract_tags(text, topK=20, withWeight=True, allowPOS=('nr', 'ns','nt','nz'))
           print('extract_tags--------------')
           for item in kw:
               if(item  not in result1):
                   result1.append(item)
                   print(item[0], item[1])
   ```

2. `jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))`

   - 直接使用，接口相同，注意默认过滤词性。
   - 基本思想

   1. 将待抽取关键词的文本进行分词
   2. 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图
   3. 计算图中节点的PageRank，注意是无向带权图

   ```python
   print('textrank-----------------')
           kw = jieba.analyse.textrank(text,topK=20,withWeight=True,allowPOS=('nr', 'ns','nt','nz'))
           for item in kw:
               if(item  not in result2):
                   result2.append(item)
                   print(item[0], item[1])
   ```

3. 所有的代码

   ```python
   import jieba
   import jieba.analyse
   import jieba.posseg as posg
   import os
   
   
   # files=['121.txt','171.txt']
   path="D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\分词实验预处理后数据\\"
   files=os.listdir(path)
   for file in files:
       print(file)
       result1=[]
       result2=[]
       with open(path+file,encoding='ansi') as txt:
           text=txt.read()
           kw = jieba.analyse.extract_tags(text, topK=20, withWeight=True, allowPOS=('nr', 'ns','nt','nz'))
           print('extract_tags--------------')
           for item in kw:
               if(item  not in result1):
                   result1.append(item)
                   print(item[0], item[1])
                   
           print('textrank-----------------')
           kw = jieba.analyse.textrank(text,topK=20,withWeight=True,allowPOS=('nr', 'ns','nt','nz'))
           for item in kw:
               if(item  not in result2):
                   result2.append(item)
                   print(item[0], item[1])
   ```

4. 结果截图

   ![image-20220407112900575](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220407112900575.png)


### 进一步错误分析

> 要求：并人工标注分词实验中的人工标注的那2篇新闻的命名实体识别的结果，计算命名实体识别准确率（正确命名实体数目/这2篇新闻的人工标注命名实体总数），进一步做错误分析

1. 首先对人工按行进行关键字提取

   ```python
   def man_make_result_word(line):
       from snownlp import SnowNLP
       s = SnowNLP(line)
       a=s.keywords(10)
       return a
   ```

2. 用jieba工具进行关键字提取

   ```python
   kw = jieba.analyse.extract_tags(line, topK=20, withWeight=True, allowPOS=('nr', 'ns','nt','nz'))
   ```

3. 统计数据，其中`result`是用来表示人工分词的按行提取关键字的结果，`result1`避免输出重复的数据

   ```python
   # result=man_make_result_word(line)
   # result1=[]
   for item in kw:
       if item[0] in result:
           sum+=1
        if item not in result1:
           result1.append(item)
           print(item[0],item[1])
   ```

4. 输出正确率

   ```python
   print('正确率：',sum/manmake_sum)
   ```

5. 全部代码

   ```python
   def man_make_result_word(line):
       from snownlp import SnowNLP
       s = SnowNLP(line)
       a=s.keywords(10)
       return a
   
   import jieba
   import jieba.analyse
   import jieba.posseg as posg
   import os
   
   files=['121.txt','171.txt']
   path="D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\人工分词和分析\\"
   for file in files:
       print(file)
       # 人工切分的结果
       result= []
       manmake_sum=0
       sum=0
       
       result1=[]
       with open(path+file,encoding='ansi') as txt:
           text=txt.read()
           text_line=text.split()
           for line in text_line:
               result=man_make_result_word(line)
               manmake_sum+=len(result)
               kw = jieba.analyse.extract_tags(line, topK=20, withWeight=True, allowPOS=('nr', 'ns','nt','nz'))
               for item in kw:
                   if item[0] in result:
                       sum+=1
                   if item not in result1:
                       result1.append(item)
                       print(item[0],item[1])
           print('正确率：',sum/manmake_sum)
   
   ```

6. 输出结果

   ![image-20220408125910226](%E9%99%88%E5%88%9B%E6%85%A7201941417201%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.assets/image-20220408125910226.png)

### 附加题

> 要求：【附加题】用课件中HMM算法[可参考](https://www.cnblogs.com/lokvahkoor/p/12781056.html)来进行命名实体识别的模型训练与测试，【训练数据可以用：`实验数据/第5章/1980_01rmrb.txt`，已上传在005-006-实验代码参考与数据集.zip】并对比出jieba与HMM模型各自或者共同的实体识别错误样例；

```python
class HMM(object):
    def __init__(self):
        pass
    
    def try_load_model(self, trained):
        pass
    
    def train(self, path):
        pass
    
    def viterbi(self, text, states, start_p, trans_p, emit_p):
        pass
    
    def cut(self, text):
        pass
```

```python
class HMM(object):
    def __init__(self):
        import os

        # 主要是用于存取算法中间结果，不用每次都训练模型
        self.model_file = 'D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\实验分词所用词典-dict.txt.big\\hmm_model.pkl'

        # 状态值集合
        self.state_list = ['B', 'M', 'E', 'S']
        # 参数加载,用于判断是否需要重新加载model_file
        self.load_para = False

    # 用于加载已计算的中间结果，当需要重新训练时，需初始化清空结果
    def try_load_model(self, trained):
        if trained:
            import pickle
            with open(self.model_file, 'rb') as f:
                self.A_dic = pickle.load(f)
                self.B_dic = pickle.load(f)
                self.Pi_dic = pickle.load(f)
                self.load_para = True

        else:
            # 状态转移概率（状态->状态的条件概率）
            self.A_dic = {}
            # 发射概率（状态->词语的条件概率）
            self.B_dic = {}
            # 状态的初始概率
            self.Pi_dic = {}
            self.load_para = False

    # 计算转移概率、发射概率以及初始概率
    def train(self, path):

        # 重置几个概率矩阵
        self.try_load_model(False)

        # 统计状态出现次数，求p(o)
        Count_dic = {}

        # 初始化参数
        def init_parameters():
            for state in self.state_list:
                self.A_dic[state] = {s: 0.0 for s in self.state_list}
                self.Pi_dic[state] = 0.0
                self.B_dic[state] = {}

                Count_dic[state] = 0

        def makeLabel(text):
            out_text = []
            if len(text) == 1:
                out_text.append('S')
            else:
                out_text += ['B'] + ['M'] * (len(text) - 2) + ['E']

            return out_text

        init_parameters()
        line_num = -1
        # 观察者集合，主要是字以及标点等
        words = set()
        with open(path, encoding='utf8') as f:
            for line in f:
                line_num += 1

                line = line.strip()
                if not line:
                    continue

                word_list = [i for i in line if i != ' ']
                words |= set(word_list)  # 更新字的集合

                linelist = line.split()

                line_state = []
                for w in linelist:
                    line_state.extend(makeLabel(w))
                
                assert len(word_list) == len(line_state)

                for k, v in enumerate(line_state):
                    Count_dic[v] += 1
                    if k == 0:
                        self.Pi_dic[v] += 1  # 每个句子的第一个字的状态，用于计算初始状态概率
                    else:
                        self.A_dic[line_state[k - 1]][v] += 1  # 计算转移概率
                        self.B_dic[line_state[k]][word_list[k]] = \
                            self.B_dic[line_state[k]].get(word_list[k], 0) + 1.0  # 计算发射概率
        
        self.Pi_dic = {k: v * 1.0 / line_num for k, v in self.Pi_dic.items()}
        self.A_dic = {k: {k1: v1 / Count_dic[k] for k1, v1 in v.items()}
                      for k, v in self.A_dic.items()}
        #加1平滑
        self.B_dic = {k: {k1: (v1 + 1) / Count_dic[k] for k1, v1 in v.items()}
                      for k, v in self.B_dic.items()}
        #序列化
        import pickle
        with open(self.model_file, 'wb') as f:
            pickle.dump(self.A_dic, f)
            pickle.dump(self.B_dic, f)
            pickle.dump(self.Pi_dic, f)

        return self

    def viterbi(self, text, states, start_p, trans_p, emit_p):
        V = [{}]
        path = {}
        for y in states:
            V[0][y] = start_p[y] * emit_p[y].get(text[0], 0)
            path[y] = [y]
        for t in range(1, len(text)):
            V.append({})
            newpath = {}
            
            #检验训练的发射概率矩阵中是否有该字
            neverSeen = text[t] not in emit_p['S'].keys() and \
                text[t] not in emit_p['M'].keys() and \
                text[t] not in emit_p['E'].keys() and \
                text[t] not in emit_p['B'].keys()
            for y in states:
                emitP = emit_p[y].get(text[t], 0) if not neverSeen else 1.0 #设置未知字单独成词
                (prob, state) = max(
                    [(V[t - 1][y0] * trans_p[y0].get(y, 0) *
                      emitP, y0)
                     for y0 in states if V[t - 1][y0] > 0])
                V[t][y] = prob
                newpath[y] = path[state] + [y]
            path = newpath
            
        if emit_p['M'].get(text[-1], 0)> emit_p['S'].get(text[-1], 0):
            (prob, state) = max([(V[len(text) - 1][y], y) for y in ('E','M')])
        else:
            (prob, state) = max([(V[len(text) - 1][y], y) for y in states])
        
        return (prob, path[state])

    def cut(self, text):
        import os
        if not self.load_para:
            self.try_load_model(os.path.exists(self.model_file))
        prob, pos_list = self.viterbi(text, self.state_list, self.Pi_dic, self.A_dic, self.B_dic)      
        begin, next = 0, 0    
        for i, char in enumerate(text):
            pos = pos_list[i]
            if pos == 'B':
                begin = i
            elif pos == 'E':
                yield text[begin: i+1]
                next = i+1
            elif pos == 'S':
                yield char
                next = i+1
        if next < len(text):
            yield text[next:]


```

```python
hmm = HMM()
hmm.train('D:\\workspaces\\AI\\ip\\人工智能\\第二次实验数据\\实验代码参考与数据集\\实验分词所用词典-dict.txt.big\\trainCorpus.txt_utf8')

text = '这是一个非常棒的方案！'
res = hmm.cut(text)
print(text)
print(str(list(res)))
```



